{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy data generator for Lake Databases, generated and managed from the Synapse workspace modeller\r\n",
        "## Attention: As designed, the data existing in that database folders will be **deleted**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\r\n",
        "A Synapse Workspace with an **existing Lake Database**. Creating that database from the lake database templates is a simple manner to generate a good starting point for a dummy environment. \r\n",
        "\r\n",
        "A Spark pool with the **faker python library**\r\n",
        " installed (simplest method is to have a **requirements.txt** with a line of text with the library name “faker” loaded in the cluster configuration). For detailed information https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-python-packages#install-python-packages\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\r\n",
        "You must introduce the name of the lake database in the notebook first cell. Then we can execute the cells of the notebook. The first code cell contains configuration and libraries. The second code cell contains the python class definition, the creation of one instance of the generator and its execution"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, string\r\n",
        "import datetime as dtime\r\n",
        "import re   \r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "import time\r\n",
        "import pandas as pd\r\n",
        "import requests, json\r\n",
        "import faker\r\n",
        "import math\r\n",
        "\r\n",
        "# Please fill the name of the database in the following line\r\n",
        "database = 'mydatabase'\r\n",
        "tables = spark.sql('show tables in ' + database)\r\n",
        "print(\"Tables number: \" + str(tables.count()))\r\n",
        "\r\n",
        "pd.set_option('display.max_rows', 20)\r\n",
        "pd.set_option('display.max_columns', None)\r\n",
        "pd.set_option('display.width', None)\r\n",
        "pd.set_option('display.max_colwidth', None)\r\n",
        "seed = math.floor(time.time())%20000"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here onwards we define a class with everything needed to populate lake DB.\r\n",
        "# Last 3 lines of the cell creates an instance of the class and executes the generation for the whole database.\r\n",
        "class templateDataGenerator():\r\n",
        "    databasename = \"\"\r\n",
        "    def __init__(self,database):\r\n",
        "        self.databasename = database\r\n",
        "        self.enrichTablesData()\r\n",
        "        return\r\n",
        "    def enrichTablesData(self):\r\n",
        "        entries = []\r\n",
        "        baseurl=\"https://\"+mssparkutils.env.getWorkspaceName()+\".dev.azuresynapse.net\"\r\n",
        "        headers = {\"Authorization\": \"Bearer \"+mssparkutils.credentials.getToken(\"Synapse\")}\r\n",
        "        constructed_url = baseurl + \"/databases/\"+database+\"/tables?api-version=2021-04-01\"\r\n",
        "        request = requests.get(constructed_url, headers=headers)\r\n",
        "        response = request.json()\r\n",
        "        results = []\r\n",
        "        for item in response['items']:\r\n",
        "            if item['type'] == 'TABLE':\r\n",
        "                tdict = {}\r\n",
        "                tdict['tableName'] = item['name']\r\n",
        "                tdict['database'] = database\r\n",
        "                columns=[]\r\n",
        "                for column in item['properties']['StorageDescriptor']['Columns']:\r\n",
        "                    cdict = {}                   \r\n",
        "                    cdict['colname'] = column['Name']\r\n",
        "                    cdatatype=column['OriginDataTypeName']\r\n",
        "                    cdict['type']=cdatatype['TypeName']\r\n",
        "                    cdict['length'] = cdatatype['Length']\r\n",
        "                    columns.append(cdict)\r\n",
        "                tdict['columns'] = columns\r\n",
        "                results.append(tdict)\r\n",
        "        self.schemaDic = results\r\n",
        "        for table in self.schemaDic:\r\n",
        "            tname = table['tableName']\r\n",
        "            if tname.__contains__('Transaction') or tname.__contains__('Order'):\r\n",
        "                sizeorder = 100\r\n",
        "            elif tname.__contains__('Type') or tname.__contains__('Method') or tname.__contains__('Channel') or tname.__contains__('Class') :\r\n",
        "                sizeorder = 1\r\n",
        "            else:\r\n",
        "                sizeorder = 10\r\n",
        "            # This case/swithc like looks for some string in the name to identify the nature of some tables. \r\n",
        "            # The transaction like should be high freq events. \r\n",
        "            # type and categories should be like 100 times smaller. \r\n",
        "            # Other tables will be in between\r\n",
        "            colcount,skcount = 0,0\r\n",
        "            pkname = \"\"\r\n",
        "            sknames = []\r\n",
        "            for column in table['columns']:\r\n",
        "                colcount += 1\r\n",
        "                name = column['colname']\r\n",
        "                if name.endswith('Id'):\r\n",
        "                    if name == (tname + 'Id'):\r\n",
        "                        pkname = name\r\n",
        "                    else:\r\n",
        "                        sknames.append(name)\r\n",
        "                        skcount += 1\r\n",
        "            table['skcount'] = skcount, \r\n",
        "            table['sizeorder'] = sizeorder\r\n",
        "        return\r\n",
        "    def calculatedecimal(self,datatype): \r\n",
        "        matches = re.findall(r'decimal(\\b,\\b)', datatype)\r\n",
        "        if len(matches) == 2:\r\n",
        "            precision = matches[0]\r\n",
        "            scale = matches[1]\r\n",
        "        else:\r\n",
        "            precision = 4\r\n",
        "            scale = 2\r\n",
        "        return precision,scale\r\n",
        "\r\n",
        "    def createtypes(self, dtype, table, name, dlength, dfix):\r\n",
        "        global seed\r\n",
        "        # dtype = Data type for the column\r\n",
        "        # table = tablename\r\n",
        "        # name = column name\r\n",
        "        # dfix = Dataframe filled with ids with the size expected.\r\n",
        "        aseed = random.randint(0,seed)\r\n",
        "        seed = (seed + aseed)%20003\r\n",
        "        faker.Faker.seed(aseed)\r\n",
        "        if dtype == 'integer' or dtype == 'long':\r\n",
        "            if name == (table + 'Id'):\r\n",
        "                # this column must be primary key, then we fill it with integer sequence == id, which is a sequence\r\n",
        "                valuesdf = dfix.select(\"id\", col(\"id\").alias(name))\r\n",
        "            elif name.endswith('Id'):\r\n",
        "                # This is a secondary key.\r\n",
        "                # some of the secondary tables can exist or not\r\n",
        "                # we try/except to check if exists.\r\n",
        "                try:\r\n",
        "                    mydf = self.tablesDf\r\n",
        "                    sksize = mydf.filter(mydf['tablename'] == name[:-2]).first()['nrows']\r\n",
        "                    valuesdf = dfix.select(\"id\", floor(rand(seed=aseed)*sksize).alias(name))\r\n",
        "                except:\r\n",
        "                    # in case we do not know the foreign table size we generate a simple sequence == primary key\r\n",
        "                    valuesdf = dfix.select(\"id\", col(\"id\").alias(name))\r\n",
        "                # creates a dataframe with the id columsn and a column identified with 'name'\r\n",
        "            else:\r\n",
        "                valuesdf = dfix.select(\"id\", floor(rand(seed=aseed)*256).alias(name))\r\n",
        "                #random values from 0 to 256 integers\r\n",
        "        elif dtype == 'string':\r\n",
        "            if name.__contains__('Url'):\r\n",
        "                fake = faker.Faker()\r\n",
        "                udf_fake = udf(fake.image_url)\r\n",
        "                valuesdf = dfix.select('id',udf_fake().alias(name))\r\n",
        "            elif name.__contains__('Email'):\r\n",
        "                fake = faker.Faker()\r\n",
        "                udf_fake = udf(fake.email)\r\n",
        "                valuesdf = dfix.select('id',udf_fake().alias(name))\r\n",
        "            elif name.__contains__('IpAddress'):\r\n",
        "                fake = faker.Faker()\r\n",
        "                udf_fake = udf(fake.ipv4)\r\n",
        "                valuesdf = dfix.select('id',udf_fake().alias(name))\r\n",
        "            elif name.__contains__('EmployeeName'):\r\n",
        "                fake = faker.Faker()\r\n",
        "                udf_fake = udf(fake.name)\r\n",
        "                valuesdf = dfix.select('id',udf_fake().alias(name))\r\n",
        "            else:\r\n",
        "                nrows = dfix.count()\r\n",
        "                fake = faker.Faker()\r\n",
        "                def mycharfake():\r\n",
        "                    return random.choice(string.ascii_lowercase)\r\n",
        "                def mytextfake():\r\n",
        "                    return fake.text(max_nb_chars=dlength)\r\n",
        "                def mylettersfake():\r\n",
        "                    return fake.random_letters(length=dlength)\r\n",
        "                if dlength == 1:\r\n",
        "                    udf_fake = udf(mycharfake)\r\n",
        "                elif dlength > 5:\r\n",
        "                    udf_fake = udf(mytextfake)\r\n",
        "                else:\r\n",
        "                    udf_fake = udf(mylettersfake)\r\n",
        "                valuesdf = dfix.select(\"id\",udf_fake().alias(name))\r\n",
        "        elif dtype == 'binary':\r\n",
        "            valuesdf = dfix.select(\"id\", (rand(seed=aseed)))\r\n",
        "            valuesdf = valuesdf.rdd.map(lambda x: (x[0], bytearray(str(x[1]),\"UTF-8\"))).toDF([\"id\",name])\r\n",
        "        elif dtype == 'date':\r\n",
        "            valuesdf = dfix.select(\"id\", floor(rand(seed=aseed)*500)) # random integer 0-500\r\n",
        "            today = dtime.date.today()\r\n",
        "            valuesdf = valuesdf.rdd.map(lambda x: (x[0], today-dtime.timedelta(days=x[1]))).toDF([\"id\",name])\r\n",
        "        elif dtype == 'bigint':\r\n",
        "            valuesdf = dfix.select(\"id\", floor(rand(seed=aseed)*3650000000).alias(name)) # random integers\r\n",
        "        elif dtype == 'timestamp':\r\n",
        "            dtoday =  time.time()\r\n",
        "            valuesdf = dfix.select(\"id\", floor(rand(seed=aseed)*15000000))\r\n",
        "            valuesdf = valuesdf.rdd.map(lambda x: (x[0], dtoday-x[1])).toDF([\"id\",\"unixtime\"])\r\n",
        "            valuesdf = valuesdf.select(\"id\",to_timestamp(from_unixtime(col(\"unixtime\"))).alias(name))\r\n",
        "        elif dtype == 'boolean':\r\n",
        "            valuesdf = dfix.select(\"id\", (rand(seed=aseed) > 0.5 ).alias(name))\r\n",
        "        elif 'decimal' in dtype:\r\n",
        "            precision,scale = self.calculatedecimal(dtype)\r\n",
        "            intrange = 10^(precision-scale)-1\r\n",
        "            valuesdf = dfix.select(\"id\", round(rand(seed=aseed)*intrange*2-intrange,scale).alias(name))\r\n",
        "        else:\r\n",
        "            print('ERROR: '+ dtype +' is a datatype not conforming')\r\n",
        "        return valuesdf\r\n",
        "\r\n",
        "    def calculate_db_tables(self, size=5000):\r\n",
        "        for table in self.schemaDic:\r\n",
        "            rows = math.floor(table['sizeorder']*(size*(.9+random.random()/5)))\r\n",
        "            table['nrows'] = rows\r\n",
        "            self.populate_table_df(table,rows)\r\n",
        "            print('Table: ' + table['tableName'] + ' -- Rows: ' + str(table['nrows']))\r\n",
        "        return\r\n",
        "\r\n",
        "    def populate_table_df(self, tabledict, size = 5000):\r\n",
        "        emptydf = sqlContext.range(size)\r\n",
        "        mydf = sqlContext.range(size)\r\n",
        "        for column in tabledict['columns']:\r\n",
        "            columndf=self.createtypes(column['type'], tabledict['tableName'], column['colname'], column['length'], emptydf)\r\n",
        "            mydf=mydf.join(columndf,'id','left')\r\n",
        "            # column[1] is tablename, column[0] is columname\r\n",
        "        output = mydf\r\n",
        "        output = output.drop('id')\r\n",
        "        # We delete the original id column, although it could be the PK, we just avoid it, as there might not be PK in a table\r\n",
        "        # coallesce to write a limited minimum file size which is fine for most tables with aprox 50K rows per file/worker\r\n",
        "        if size < 80000:\r\n",
        "            output.repartition(1).write.insertInto(database+'.'+tabledict['tableName'],overwrite = True)\r\n",
        "        elif size < 800000:\r\n",
        "            output.repartition(5).write.insertInto(database+'.'+tabledict['tableName'],overwrite = True)\r\n",
        "        else:\r\n",
        "            output.repartition(50).write.insertInto(database+'.'+tabledict['tableName'],overwrite = True)\r\n",
        "        return\r\n",
        "\r\n",
        "\r\n",
        "mydatagen=templateDataGenerator(database)\r\n",
        "#display(mydatagen.tablesDf)\r\n",
        "mydatagen.calculate_db_tables(size = 10000)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}